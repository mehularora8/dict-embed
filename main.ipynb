{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5b96c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 1)) (4.57.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 1)) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 1)) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 1)) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 1)) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 1)) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 1)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from tqdm->sentence-transformers->-r requirements.txt (line 1)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 1)) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 1)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 1)) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2441c8eb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "917aa221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data/csw24.txt and convert line by line to csv\n",
    "# each line is word   definition.\n",
    "# convert to csv with two columns: word and definition\n",
    "import pandas as pd\n",
    "\n",
    "with open('data/csw24.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# each line is word<tab>definition.\n",
    "# convert to csv with two columns: word and definition\n",
    "data = []\n",
    "for line in lines:\n",
    "    word, definition = line.strip().split('\\t', 1)\n",
    "    data.append({'word': word, 'definition': definition})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "assert len(df) == len(lines)\n",
    "df.to_csv('data/csw24.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68854f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses, SentenceTransformerTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "# model.max_seq_length = 256 # For trial run purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd829d",
   "metadata": {},
   "source": [
    "## Matryoshka Loss\n",
    "\n",
    "Matryoshka Representation Learning trains embeddings at multiple dimensions simultaneously, ensuring smaller embeddings (prefixes of the full embedding) remain useful while optimizing the full dimension. This provides flexible quality/speed tradeoffs from a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "target_dims = [384, 256]\n",
    "mrl_loss = losses.MatryoshkaLoss(model, base_loss, target_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e947159",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The CSW24 dictionary dataset contains word-definition pairs used for training. The dataset is split into train/validation/test sets for model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOCATION = \"\"\n",
    "dataset = load_dataset(\"csv\", data_files=DATA_LOCATION)\n",
    "\n",
    "# Split into train and temp (test+val)\n",
    "splits = dataset['train'].train_test_split(test_size=0.2)  # 80% train, 20% temp\n",
    "train_dataset = splits['train']\n",
    "temp = splits['test']\n",
    "\n",
    "# Split temp into val and test\n",
    "temp_splits = temp.train_test_split(test_size=0.5)  # 50% val, 50% test\n",
    "val_dataset = temp_splits['train']\n",
    "test_dataset = temp_splits['test']\n",
    "\n",
    "print(\"Train Dataset Size:\", len(train_dataset))\n",
    "print(\"Val Dataset Size:\", len(val_dataset))\n",
    "print(\"Test Dataset Size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f7f72",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "The InformationRetrievalEvaluator measures how well the model retrieves the correct definition for each word using cosine similarity. It computes accuracy, precision, recall, and NDCG metrics at various top-K thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = InformationRetrievalEvaluator(\n",
    "    queries={i: example['word'] for i, example in enumerate(val_dataset)},\n",
    "    corpus={i: example['definition'] for i, example in enumerate(val_dataset)},\n",
    "    relevant_docs={i: [i] for i in range(len(val_dataset))}, # Word i's def is always doc i\n",
    "    name='dictionary-test'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8423cc",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "The SentenceTransformerTrainer handles the training loop with the specified loss function, training arguments, and evaluator. It automatically manages batching, gradient updates, evaluation, and checkpointing during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92175f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir='./output',\n",
    "    per_device_train_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-5,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=mrl_loss,\n",
    "    args=training_args,\n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e8059a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arora\\OneDrive\\Desktop\\Dev\\scrabble-embed\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3555' max='3555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3555/3555 1:15:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Dictionary-test Cosine Accuracy@1</th>\n",
       "      <th>Dictionary-test Cosine Accuracy@3</th>\n",
       "      <th>Dictionary-test Cosine Accuracy@5</th>\n",
       "      <th>Dictionary-test Cosine Accuracy@10</th>\n",
       "      <th>Dictionary-test Cosine Precision@1</th>\n",
       "      <th>Dictionary-test Cosine Precision@3</th>\n",
       "      <th>Dictionary-test Cosine Precision@5</th>\n",
       "      <th>Dictionary-test Cosine Precision@10</th>\n",
       "      <th>Dictionary-test Cosine Recall@1</th>\n",
       "      <th>Dictionary-test Cosine Recall@3</th>\n",
       "      <th>Dictionary-test Cosine Recall@5</th>\n",
       "      <th>Dictionary-test Cosine Recall@10</th>\n",
       "      <th>Dictionary-test Cosine Ndcg@10</th>\n",
       "      <th>Dictionary-test Cosine Mrr@10</th>\n",
       "      <th>Dictionary-test Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.535300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>0.670570</td>\n",
       "      <td>0.709771</td>\n",
       "      <td>0.743513</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>0.223523</td>\n",
       "      <td>0.141954</td>\n",
       "      <td>0.074351</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>0.670570</td>\n",
       "      <td>0.709771</td>\n",
       "      <td>0.743513</td>\n",
       "      <td>0.630627</td>\n",
       "      <td>0.593713</td>\n",
       "      <td>0.596473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.283600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.691377</td>\n",
       "      <td>0.720807</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.230459</td>\n",
       "      <td>0.144161</td>\n",
       "      <td>0.074854</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.691377</td>\n",
       "      <td>0.720807</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.654278</td>\n",
       "      <td>0.623322</td>\n",
       "      <td>0.626132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.230500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.559217</td>\n",
       "      <td>0.698972</td>\n",
       "      <td>0.727532</td>\n",
       "      <td>0.755617</td>\n",
       "      <td>0.559217</td>\n",
       "      <td>0.232991</td>\n",
       "      <td>0.145506</td>\n",
       "      <td>0.075562</td>\n",
       "      <td>0.559217</td>\n",
       "      <td>0.698972</td>\n",
       "      <td>0.727532</td>\n",
       "      <td>0.755617</td>\n",
       "      <td>0.663688</td>\n",
       "      <td>0.633532</td>\n",
       "      <td>0.636208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.166900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.560680</td>\n",
       "      <td>0.700356</td>\n",
       "      <td>0.729945</td>\n",
       "      <td>0.757081</td>\n",
       "      <td>0.560680</td>\n",
       "      <td>0.233452</td>\n",
       "      <td>0.145989</td>\n",
       "      <td>0.075708</td>\n",
       "      <td>0.560680</td>\n",
       "      <td>0.700356</td>\n",
       "      <td>0.729945</td>\n",
       "      <td>0.757081</td>\n",
       "      <td>0.665149</td>\n",
       "      <td>0.634996</td>\n",
       "      <td>0.637667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.190400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.572271</td>\n",
       "      <td>0.704905</td>\n",
       "      <td>0.732239</td>\n",
       "      <td>0.758109</td>\n",
       "      <td>0.572271</td>\n",
       "      <td>0.234968</td>\n",
       "      <td>0.146448</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.572271</td>\n",
       "      <td>0.704905</td>\n",
       "      <td>0.732239</td>\n",
       "      <td>0.758109</td>\n",
       "      <td>0.671440</td>\n",
       "      <td>0.642986</td>\n",
       "      <td>0.645754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.099800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.573774</td>\n",
       "      <td>0.707951</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761709</td>\n",
       "      <td>0.573774</td>\n",
       "      <td>0.235984</td>\n",
       "      <td>0.146875</td>\n",
       "      <td>0.076171</td>\n",
       "      <td>0.573774</td>\n",
       "      <td>0.707951</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761709</td>\n",
       "      <td>0.673844</td>\n",
       "      <td>0.645033</td>\n",
       "      <td>0.647714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.065500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.575277</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.735285</td>\n",
       "      <td>0.762658</td>\n",
       "      <td>0.575277</td>\n",
       "      <td>0.236287</td>\n",
       "      <td>0.147057</td>\n",
       "      <td>0.076266</td>\n",
       "      <td>0.575277</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.735285</td>\n",
       "      <td>0.762658</td>\n",
       "      <td>0.675059</td>\n",
       "      <td>0.646343</td>\n",
       "      <td>0.649096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.095000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.580934</td>\n",
       "      <td>0.711472</td>\n",
       "      <td>0.735997</td>\n",
       "      <td>0.763252</td>\n",
       "      <td>0.580934</td>\n",
       "      <td>0.237157</td>\n",
       "      <td>0.147199</td>\n",
       "      <td>0.076325</td>\n",
       "      <td>0.580934</td>\n",
       "      <td>0.711472</td>\n",
       "      <td>0.735997</td>\n",
       "      <td>0.763252</td>\n",
       "      <td>0.678071</td>\n",
       "      <td>0.650149</td>\n",
       "      <td>0.652928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.153500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.585839</td>\n",
       "      <td>0.712658</td>\n",
       "      <td>0.738924</td>\n",
       "      <td>0.765427</td>\n",
       "      <td>0.585839</td>\n",
       "      <td>0.237553</td>\n",
       "      <td>0.147785</td>\n",
       "      <td>0.076543</td>\n",
       "      <td>0.585839</td>\n",
       "      <td>0.712658</td>\n",
       "      <td>0.738924</td>\n",
       "      <td>0.765427</td>\n",
       "      <td>0.681258</td>\n",
       "      <td>0.653691</td>\n",
       "      <td>0.656405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.586432</td>\n",
       "      <td>0.713331</td>\n",
       "      <td>0.737540</td>\n",
       "      <td>0.765071</td>\n",
       "      <td>0.586432</td>\n",
       "      <td>0.237777</td>\n",
       "      <td>0.147508</td>\n",
       "      <td>0.076507</td>\n",
       "      <td>0.586432</td>\n",
       "      <td>0.713331</td>\n",
       "      <td>0.737540</td>\n",
       "      <td>0.765071</td>\n",
       "      <td>0.681367</td>\n",
       "      <td>0.653972</td>\n",
       "      <td>0.656740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.074900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.583070</td>\n",
       "      <td>0.714517</td>\n",
       "      <td>0.739992</td>\n",
       "      <td>0.766495</td>\n",
       "      <td>0.583070</td>\n",
       "      <td>0.238172</td>\n",
       "      <td>0.147998</td>\n",
       "      <td>0.076650</td>\n",
       "      <td>0.583070</td>\n",
       "      <td>0.714517</td>\n",
       "      <td>0.739992</td>\n",
       "      <td>0.766495</td>\n",
       "      <td>0.680913</td>\n",
       "      <td>0.652843</td>\n",
       "      <td>0.655654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.064200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.586353</td>\n",
       "      <td>0.712737</td>\n",
       "      <td>0.738687</td>\n",
       "      <td>0.764399</td>\n",
       "      <td>0.586353</td>\n",
       "      <td>0.237579</td>\n",
       "      <td>0.147737</td>\n",
       "      <td>0.076440</td>\n",
       "      <td>0.586353</td>\n",
       "      <td>0.712737</td>\n",
       "      <td>0.738687</td>\n",
       "      <td>0.764399</td>\n",
       "      <td>0.681294</td>\n",
       "      <td>0.654044</td>\n",
       "      <td>0.656927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.071800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.590190</td>\n",
       "      <td>0.717049</td>\n",
       "      <td>0.742880</td>\n",
       "      <td>0.768275</td>\n",
       "      <td>0.590190</td>\n",
       "      <td>0.239016</td>\n",
       "      <td>0.148576</td>\n",
       "      <td>0.076828</td>\n",
       "      <td>0.590190</td>\n",
       "      <td>0.717049</td>\n",
       "      <td>0.742880</td>\n",
       "      <td>0.768275</td>\n",
       "      <td>0.685073</td>\n",
       "      <td>0.657792</td>\n",
       "      <td>0.660538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.023000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.588410</td>\n",
       "      <td>0.719581</td>\n",
       "      <td>0.744343</td>\n",
       "      <td>0.770095</td>\n",
       "      <td>0.588410</td>\n",
       "      <td>0.239860</td>\n",
       "      <td>0.148869</td>\n",
       "      <td>0.077009</td>\n",
       "      <td>0.588410</td>\n",
       "      <td>0.719581</td>\n",
       "      <td>0.744343</td>\n",
       "      <td>0.770095</td>\n",
       "      <td>0.685427</td>\n",
       "      <td>0.657646</td>\n",
       "      <td>0.660365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.042900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.589953</td>\n",
       "      <td>0.717286</td>\n",
       "      <td>0.743157</td>\n",
       "      <td>0.768552</td>\n",
       "      <td>0.589953</td>\n",
       "      <td>0.239095</td>\n",
       "      <td>0.148631</td>\n",
       "      <td>0.076855</td>\n",
       "      <td>0.589953</td>\n",
       "      <td>0.717286</td>\n",
       "      <td>0.743157</td>\n",
       "      <td>0.768552</td>\n",
       "      <td>0.685031</td>\n",
       "      <td>0.657651</td>\n",
       "      <td>0.660485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.008800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.587777</td>\n",
       "      <td>0.718315</td>\n",
       "      <td>0.744541</td>\n",
       "      <td>0.769541</td>\n",
       "      <td>0.587777</td>\n",
       "      <td>0.239438</td>\n",
       "      <td>0.148908</td>\n",
       "      <td>0.076954</td>\n",
       "      <td>0.587777</td>\n",
       "      <td>0.718315</td>\n",
       "      <td>0.744541</td>\n",
       "      <td>0.769541</td>\n",
       "      <td>0.684857</td>\n",
       "      <td>0.657055</td>\n",
       "      <td>0.659855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.012900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.594778</td>\n",
       "      <td>0.718434</td>\n",
       "      <td>0.743315</td>\n",
       "      <td>0.768473</td>\n",
       "      <td>0.594778</td>\n",
       "      <td>0.239478</td>\n",
       "      <td>0.148663</td>\n",
       "      <td>0.076847</td>\n",
       "      <td>0.594778</td>\n",
       "      <td>0.718434</td>\n",
       "      <td>0.743315</td>\n",
       "      <td>0.768473</td>\n",
       "      <td>0.687287</td>\n",
       "      <td>0.660670</td>\n",
       "      <td>0.663517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.988000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.592366</td>\n",
       "      <td>0.720174</td>\n",
       "      <td>0.744976</td>\n",
       "      <td>0.770214</td>\n",
       "      <td>0.592366</td>\n",
       "      <td>0.240058</td>\n",
       "      <td>0.148995</td>\n",
       "      <td>0.077021</td>\n",
       "      <td>0.592366</td>\n",
       "      <td>0.720174</td>\n",
       "      <td>0.744976</td>\n",
       "      <td>0.770214</td>\n",
       "      <td>0.687390</td>\n",
       "      <td>0.660211</td>\n",
       "      <td>0.663038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.041300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593513</td>\n",
       "      <td>0.721361</td>\n",
       "      <td>0.746400</td>\n",
       "      <td>0.771440</td>\n",
       "      <td>0.593513</td>\n",
       "      <td>0.240454</td>\n",
       "      <td>0.149280</td>\n",
       "      <td>0.077144</td>\n",
       "      <td>0.593513</td>\n",
       "      <td>0.721361</td>\n",
       "      <td>0.746400</td>\n",
       "      <td>0.771440</td>\n",
       "      <td>0.688237</td>\n",
       "      <td>0.660951</td>\n",
       "      <td>0.663748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593157</td>\n",
       "      <td>0.722508</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.771479</td>\n",
       "      <td>0.593157</td>\n",
       "      <td>0.240836</td>\n",
       "      <td>0.149343</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.593157</td>\n",
       "      <td>0.722508</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.771479</td>\n",
       "      <td>0.688519</td>\n",
       "      <td>0.661268</td>\n",
       "      <td>0.664065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.992900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593592</td>\n",
       "      <td>0.721479</td>\n",
       "      <td>0.747033</td>\n",
       "      <td>0.771559</td>\n",
       "      <td>0.593592</td>\n",
       "      <td>0.240493</td>\n",
       "      <td>0.149407</td>\n",
       "      <td>0.077156</td>\n",
       "      <td>0.593592</td>\n",
       "      <td>0.721479</td>\n",
       "      <td>0.747033</td>\n",
       "      <td>0.771559</td>\n",
       "      <td>0.688641</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.664189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.940300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.720886</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.771677</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.240295</td>\n",
       "      <td>0.149343</td>\n",
       "      <td>0.077168</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.720886</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.771677</td>\n",
       "      <td>0.689853</td>\n",
       "      <td>0.663022</td>\n",
       "      <td>0.665813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.978900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596282</td>\n",
       "      <td>0.723062</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.773101</td>\n",
       "      <td>0.596282</td>\n",
       "      <td>0.241021</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>0.077310</td>\n",
       "      <td>0.596282</td>\n",
       "      <td>0.723062</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.773101</td>\n",
       "      <td>0.690744</td>\n",
       "      <td>0.663716</td>\n",
       "      <td>0.666452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.959500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.597547</td>\n",
       "      <td>0.723022</td>\n",
       "      <td>0.747943</td>\n",
       "      <td>0.772983</td>\n",
       "      <td>0.597547</td>\n",
       "      <td>0.241007</td>\n",
       "      <td>0.149589</td>\n",
       "      <td>0.077298</td>\n",
       "      <td>0.597547</td>\n",
       "      <td>0.723022</td>\n",
       "      <td>0.747943</td>\n",
       "      <td>0.772983</td>\n",
       "      <td>0.691222</td>\n",
       "      <td>0.664399</td>\n",
       "      <td>0.667179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.978600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.723813</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.241271</td>\n",
       "      <td>0.149707</td>\n",
       "      <td>0.077354</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.723813</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.691413</td>\n",
       "      <td>0.664456</td>\n",
       "      <td>0.667252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.723497</td>\n",
       "      <td>0.748655</td>\n",
       "      <td>0.773457</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.241166</td>\n",
       "      <td>0.149731</td>\n",
       "      <td>0.077346</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.723497</td>\n",
       "      <td>0.748655</td>\n",
       "      <td>0.773457</td>\n",
       "      <td>0.691061</td>\n",
       "      <td>0.664013</td>\n",
       "      <td>0.666813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.924500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593869</td>\n",
       "      <td>0.723378</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.772587</td>\n",
       "      <td>0.593869</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>0.077259</td>\n",
       "      <td>0.593869</td>\n",
       "      <td>0.723378</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.772587</td>\n",
       "      <td>0.689694</td>\n",
       "      <td>0.662437</td>\n",
       "      <td>0.665314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.968500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.723932</td>\n",
       "      <td>0.748734</td>\n",
       "      <td>0.773774</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.241311</td>\n",
       "      <td>0.149747</td>\n",
       "      <td>0.077377</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.723932</td>\n",
       "      <td>0.748734</td>\n",
       "      <td>0.773774</td>\n",
       "      <td>0.690560</td>\n",
       "      <td>0.663225</td>\n",
       "      <td>0.666030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.592801</td>\n",
       "      <td>0.723695</td>\n",
       "      <td>0.748259</td>\n",
       "      <td>0.773616</td>\n",
       "      <td>0.592801</td>\n",
       "      <td>0.241232</td>\n",
       "      <td>0.149652</td>\n",
       "      <td>0.077362</td>\n",
       "      <td>0.592801</td>\n",
       "      <td>0.723695</td>\n",
       "      <td>0.748259</td>\n",
       "      <td>0.773616</td>\n",
       "      <td>0.689553</td>\n",
       "      <td>0.661936</td>\n",
       "      <td>0.664752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.939000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.595293</td>\n",
       "      <td>0.723892</td>\n",
       "      <td>0.748576</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.595293</td>\n",
       "      <td>0.241297</td>\n",
       "      <td>0.149715</td>\n",
       "      <td>0.077354</td>\n",
       "      <td>0.595293</td>\n",
       "      <td>0.723892</td>\n",
       "      <td>0.748576</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.690647</td>\n",
       "      <td>0.663421</td>\n",
       "      <td>0.666274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.982200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.594264</td>\n",
       "      <td>0.723932</td>\n",
       "      <td>0.749525</td>\n",
       "      <td>0.773932</td>\n",
       "      <td>0.594264</td>\n",
       "      <td>0.241311</td>\n",
       "      <td>0.149905</td>\n",
       "      <td>0.077393</td>\n",
       "      <td>0.594264</td>\n",
       "      <td>0.723932</td>\n",
       "      <td>0.749525</td>\n",
       "      <td>0.773932</td>\n",
       "      <td>0.690446</td>\n",
       "      <td>0.663011</td>\n",
       "      <td>0.665855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.003800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.595886</td>\n",
       "      <td>0.724644</td>\n",
       "      <td>0.749328</td>\n",
       "      <td>0.774090</td>\n",
       "      <td>0.595886</td>\n",
       "      <td>0.241548</td>\n",
       "      <td>0.149866</td>\n",
       "      <td>0.077409</td>\n",
       "      <td>0.595886</td>\n",
       "      <td>0.724644</td>\n",
       "      <td>0.749328</td>\n",
       "      <td>0.774090</td>\n",
       "      <td>0.691257</td>\n",
       "      <td>0.664049</td>\n",
       "      <td>0.666895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.595332</td>\n",
       "      <td>0.724723</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.773932</td>\n",
       "      <td>0.595332</td>\n",
       "      <td>0.241574</td>\n",
       "      <td>0.149913</td>\n",
       "      <td>0.077393</td>\n",
       "      <td>0.595332</td>\n",
       "      <td>0.724723</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.773932</td>\n",
       "      <td>0.690979</td>\n",
       "      <td>0.663714</td>\n",
       "      <td>0.666593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596203</td>\n",
       "      <td>0.724960</td>\n",
       "      <td>0.749407</td>\n",
       "      <td>0.774209</td>\n",
       "      <td>0.596203</td>\n",
       "      <td>0.241653</td>\n",
       "      <td>0.149881</td>\n",
       "      <td>0.077421</td>\n",
       "      <td>0.596203</td>\n",
       "      <td>0.724960</td>\n",
       "      <td>0.749407</td>\n",
       "      <td>0.774209</td>\n",
       "      <td>0.691451</td>\n",
       "      <td>0.664269</td>\n",
       "      <td>0.667130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.725277</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.774328</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.241759</td>\n",
       "      <td>0.149913</td>\n",
       "      <td>0.077433</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.725277</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.774328</td>\n",
       "      <td>0.691938</td>\n",
       "      <td>0.664875</td>\n",
       "      <td>0.667724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3555, training_loss=1.0422707043954926, metrics={'train_runtime': 4517.6472, 'train_samples_per_second': 50.362, 'train_steps_per_second': 0.787, 'total_flos': 0.0, 'train_loss': 1.0422707043954926, 'epoch': 1.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0aae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODEL_REPO=\"\"\n",
    "model.save_pretrained(FINAL_MODEL_REPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f34f6",
   "metadata": {},
   "source": [
    "## HuggingFace Hub Push\n",
    "\n",
    "The trained model is pushed to the HuggingFace Hub for sharing and deployment. This allows the model to be easily loaded and used by others using the SentenceTransformer library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf614a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec784f7fb59e451885a51e5997f138a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()\n",
    "REPO_ID = \"\"\n",
    "model.push_to_hub(repo_id=REPO_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
