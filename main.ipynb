{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e5b96c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mAudited \u001b[1m5 packages\u001b[0m \u001b[2min 284ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install sentence-transformers ipywidgets pandas datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments, SentenceTransformer, losses\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917aa221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data/csw24.txt and convert line by line to csv\n",
    "# each line is word   definition.\n",
    "# convert to csv with two columns: word and definition\n",
    "# import pandas as pd\n",
    "\n",
    "# with open('data/csw24.txt', 'r') as file:\n",
    "#     lines = file.readlines()\n",
    "\n",
    "# # each line is word<tab>definition.\n",
    "# # convert to csv with two columns: word and definition\n",
    "# data = []\n",
    "# for line in lines:\n",
    "#     word, definition = line.strip().split('\\t', 1)\n",
    "#     data.append({'word': word, 'definition': definition})\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# assert len(df) == len(lines)\n",
    "# df.to_csv('data/csw24.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108c890b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bb6d63f7e04e1c98dab913ea53f367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14282ed0783848a9ad1b8cfbaab832d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e423535008d04aaba070e72aebfbbf2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900d7374ae6d46e7b7c9bc2cbe569087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa4c4f614dc4e9b838e61009f775359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e2a6f07b614e36b3a022eec712f481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e494f85c2d8e4503a44fd40461067c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297766732f964ef9aebc963fd91a98d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c61f7c679942b48ac7b984aff628f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c9033ecf5e4a7fa147a7fee89e9fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3483f4c3b0f14ba993416bc30c90a8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "# model.max_seq_length = 256 # For trial run purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd829d",
   "metadata": {},
   "source": [
    "## Matryoshka Loss\n",
    "\n",
    "Matryoshka Representation Learning trains embeddings at multiple dimensions simultaneously, ensuring smaller embeddings (prefixes of the full embedding) remain useful while optimizing the full dimension. This provides flexible quality/speed tradeoffs from a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "target_dims = [384, 256]\n",
    "mrl_loss = losses.MatryoshkaLoss(model, base_loss, target_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e947159",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The CSW24 dictionary dataset contains word-definition pairs used for training. The dataset is split into train/validation/test sets for model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Size: 222635\n",
      "Val Dataset Size: 27829\n",
      "Test Dataset Size: 27830\n"
     ]
    }
   ],
   "source": [
    "DATA_LOCATION = \"data/non_indian_words.csv\"\n",
    "dataset = load_dataset(\"csv\", data_files=DATA_LOCATION)\n",
    "\n",
    "# Split into train and temp (test+val)\n",
    "splits = dataset['train'].train_test_split(test_size=0.2)  # 80% train, 20% temp\n",
    "train_dataset = splits['train']\n",
    "temp = splits['test']\n",
    "\n",
    "# Split temp into val and test\n",
    "temp_splits = temp.train_test_split(test_size=0.5)  # 50% val, 50% test\n",
    "val_dataset = temp_splits['train']\n",
    "test_dataset = temp_splits['test']\n",
    "\n",
    "print(\"Train Dataset Size:\", len(train_dataset))\n",
    "print(\"Val Dataset Size:\", len(val_dataset))\n",
    "print(\"Test Dataset Size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f7f72",
   "metadata": {},
   "source": [
    "## Evaluator\n",
    "\n",
    "The InformationRetrievalEvaluator measures how well the model retrieves the correct definition for each word using cosine similarity. It computes accuracy, precision, recall, and NDCG metrics at various top-K thresholds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad3bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = InformationRetrievalEvaluator(\n",
    "    queries={i: example['word'] for i, example in enumerate(val_dataset)},\n",
    "    corpus={i: example['definition'] for i, example in enumerate(val_dataset)},\n",
    "    relevant_docs={i: [i] for i in range(len(val_dataset))}, # Word i's def is always doc i\n",
    "    name='dictionary-test'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8423cc",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "The SentenceTransformerTrainer handles the training loop with the specified loss function, training arguments, and evaluator. It automatically manages batching, gradient updates, evaluation, and checkpointing during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cc2450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import accelerate; print(accelerate.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92175f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformerTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SentenceTransformerTrainer(\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     17\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     evaluator\u001b[38;5;241m=\u001b[39mevaluator\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32m<string>:140\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices, prompts, batch_sampler, multi_dataset_batch_sampler, router_mapping, learning_rate_mapping)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\arora\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\training_args.py:247\u001b[0m, in \u001b[0;36mSentenceTransformerTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__post_init__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    250\u001b[0m         BatchSamplers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_sampler\n\u001b[0;32m    251\u001b[0m     )\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_dataset_batch_sampler \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    253\u001b[0m         MultiDatasetBatchSamplers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_dataset_batch_sampler)\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_dataset_batch_sampler, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_dataset_batch_sampler\n\u001b[0;32m    256\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\arora\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1811\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1809\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1811\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torchdynamo` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `torch_compile_backend` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1817\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\arora\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:2355\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2352\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2353\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2354\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arora\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    999\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m-> 1001\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1003\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\arora\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:2225\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2226\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2227\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2228\u001b[0m         )\n\u001b[0;32m   2229\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir='./output',\n",
    "    per_device_train_batch_size=64,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-5,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=mrl_loss,\n",
    "    args=training_args,\n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8059a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arora\\OneDrive\\Desktop\\Dev\\scrabble-embed\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3555' max='3555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3555/3555 1:15:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Dictionary-test Cosine Accuracy@1</th>\n",
       "      <th>Dictionary-test Cosine Accuracy@3</th>\n",
       "      <th>Dictionary-test Cosine Accuracy@5</th>\n",
       "      <th>Dictionary-test Cosine Accuracy@10</th>\n",
       "      <th>Dictionary-test Cosine Precision@1</th>\n",
       "      <th>Dictionary-test Cosine Precision@3</th>\n",
       "      <th>Dictionary-test Cosine Precision@5</th>\n",
       "      <th>Dictionary-test Cosine Precision@10</th>\n",
       "      <th>Dictionary-test Cosine Recall@1</th>\n",
       "      <th>Dictionary-test Cosine Recall@3</th>\n",
       "      <th>Dictionary-test Cosine Recall@5</th>\n",
       "      <th>Dictionary-test Cosine Recall@10</th>\n",
       "      <th>Dictionary-test Cosine Ndcg@10</th>\n",
       "      <th>Dictionary-test Cosine Mrr@10</th>\n",
       "      <th>Dictionary-test Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.535300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>0.670570</td>\n",
       "      <td>0.709771</td>\n",
       "      <td>0.743513</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>0.223523</td>\n",
       "      <td>0.141954</td>\n",
       "      <td>0.074351</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>0.670570</td>\n",
       "      <td>0.709771</td>\n",
       "      <td>0.743513</td>\n",
       "      <td>0.630627</td>\n",
       "      <td>0.593713</td>\n",
       "      <td>0.596473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.283600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.691377</td>\n",
       "      <td>0.720807</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.230459</td>\n",
       "      <td>0.144161</td>\n",
       "      <td>0.074854</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.691377</td>\n",
       "      <td>0.720807</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.654278</td>\n",
       "      <td>0.623322</td>\n",
       "      <td>0.626132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.230500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.559217</td>\n",
       "      <td>0.698972</td>\n",
       "      <td>0.727532</td>\n",
       "      <td>0.755617</td>\n",
       "      <td>0.559217</td>\n",
       "      <td>0.232991</td>\n",
       "      <td>0.145506</td>\n",
       "      <td>0.075562</td>\n",
       "      <td>0.559217</td>\n",
       "      <td>0.698972</td>\n",
       "      <td>0.727532</td>\n",
       "      <td>0.755617</td>\n",
       "      <td>0.663688</td>\n",
       "      <td>0.633532</td>\n",
       "      <td>0.636208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.166900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.560680</td>\n",
       "      <td>0.700356</td>\n",
       "      <td>0.729945</td>\n",
       "      <td>0.757081</td>\n",
       "      <td>0.560680</td>\n",
       "      <td>0.233452</td>\n",
       "      <td>0.145989</td>\n",
       "      <td>0.075708</td>\n",
       "      <td>0.560680</td>\n",
       "      <td>0.700356</td>\n",
       "      <td>0.729945</td>\n",
       "      <td>0.757081</td>\n",
       "      <td>0.665149</td>\n",
       "      <td>0.634996</td>\n",
       "      <td>0.637667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.190400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.572271</td>\n",
       "      <td>0.704905</td>\n",
       "      <td>0.732239</td>\n",
       "      <td>0.758109</td>\n",
       "      <td>0.572271</td>\n",
       "      <td>0.234968</td>\n",
       "      <td>0.146448</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.572271</td>\n",
       "      <td>0.704905</td>\n",
       "      <td>0.732239</td>\n",
       "      <td>0.758109</td>\n",
       "      <td>0.671440</td>\n",
       "      <td>0.642986</td>\n",
       "      <td>0.645754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.099800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.573774</td>\n",
       "      <td>0.707951</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761709</td>\n",
       "      <td>0.573774</td>\n",
       "      <td>0.235984</td>\n",
       "      <td>0.146875</td>\n",
       "      <td>0.076171</td>\n",
       "      <td>0.573774</td>\n",
       "      <td>0.707951</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761709</td>\n",
       "      <td>0.673844</td>\n",
       "      <td>0.645033</td>\n",
       "      <td>0.647714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.065500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.575277</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.735285</td>\n",
       "      <td>0.762658</td>\n",
       "      <td>0.575277</td>\n",
       "      <td>0.236287</td>\n",
       "      <td>0.147057</td>\n",
       "      <td>0.076266</td>\n",
       "      <td>0.575277</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.735285</td>\n",
       "      <td>0.762658</td>\n",
       "      <td>0.675059</td>\n",
       "      <td>0.646343</td>\n",
       "      <td>0.649096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.095000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.580934</td>\n",
       "      <td>0.711472</td>\n",
       "      <td>0.735997</td>\n",
       "      <td>0.763252</td>\n",
       "      <td>0.580934</td>\n",
       "      <td>0.237157</td>\n",
       "      <td>0.147199</td>\n",
       "      <td>0.076325</td>\n",
       "      <td>0.580934</td>\n",
       "      <td>0.711472</td>\n",
       "      <td>0.735997</td>\n",
       "      <td>0.763252</td>\n",
       "      <td>0.678071</td>\n",
       "      <td>0.650149</td>\n",
       "      <td>0.652928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.153500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.585839</td>\n",
       "      <td>0.712658</td>\n",
       "      <td>0.738924</td>\n",
       "      <td>0.765427</td>\n",
       "      <td>0.585839</td>\n",
       "      <td>0.237553</td>\n",
       "      <td>0.147785</td>\n",
       "      <td>0.076543</td>\n",
       "      <td>0.585839</td>\n",
       "      <td>0.712658</td>\n",
       "      <td>0.738924</td>\n",
       "      <td>0.765427</td>\n",
       "      <td>0.681258</td>\n",
       "      <td>0.653691</td>\n",
       "      <td>0.656405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.586432</td>\n",
       "      <td>0.713331</td>\n",
       "      <td>0.737540</td>\n",
       "      <td>0.765071</td>\n",
       "      <td>0.586432</td>\n",
       "      <td>0.237777</td>\n",
       "      <td>0.147508</td>\n",
       "      <td>0.076507</td>\n",
       "      <td>0.586432</td>\n",
       "      <td>0.713331</td>\n",
       "      <td>0.737540</td>\n",
       "      <td>0.765071</td>\n",
       "      <td>0.681367</td>\n",
       "      <td>0.653972</td>\n",
       "      <td>0.656740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.074900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.583070</td>\n",
       "      <td>0.714517</td>\n",
       "      <td>0.739992</td>\n",
       "      <td>0.766495</td>\n",
       "      <td>0.583070</td>\n",
       "      <td>0.238172</td>\n",
       "      <td>0.147998</td>\n",
       "      <td>0.076650</td>\n",
       "      <td>0.583070</td>\n",
       "      <td>0.714517</td>\n",
       "      <td>0.739992</td>\n",
       "      <td>0.766495</td>\n",
       "      <td>0.680913</td>\n",
       "      <td>0.652843</td>\n",
       "      <td>0.655654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.064200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.586353</td>\n",
       "      <td>0.712737</td>\n",
       "      <td>0.738687</td>\n",
       "      <td>0.764399</td>\n",
       "      <td>0.586353</td>\n",
       "      <td>0.237579</td>\n",
       "      <td>0.147737</td>\n",
       "      <td>0.076440</td>\n",
       "      <td>0.586353</td>\n",
       "      <td>0.712737</td>\n",
       "      <td>0.738687</td>\n",
       "      <td>0.764399</td>\n",
       "      <td>0.681294</td>\n",
       "      <td>0.654044</td>\n",
       "      <td>0.656927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.071800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.590190</td>\n",
       "      <td>0.717049</td>\n",
       "      <td>0.742880</td>\n",
       "      <td>0.768275</td>\n",
       "      <td>0.590190</td>\n",
       "      <td>0.239016</td>\n",
       "      <td>0.148576</td>\n",
       "      <td>0.076828</td>\n",
       "      <td>0.590190</td>\n",
       "      <td>0.717049</td>\n",
       "      <td>0.742880</td>\n",
       "      <td>0.768275</td>\n",
       "      <td>0.685073</td>\n",
       "      <td>0.657792</td>\n",
       "      <td>0.660538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.023000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.588410</td>\n",
       "      <td>0.719581</td>\n",
       "      <td>0.744343</td>\n",
       "      <td>0.770095</td>\n",
       "      <td>0.588410</td>\n",
       "      <td>0.239860</td>\n",
       "      <td>0.148869</td>\n",
       "      <td>0.077009</td>\n",
       "      <td>0.588410</td>\n",
       "      <td>0.719581</td>\n",
       "      <td>0.744343</td>\n",
       "      <td>0.770095</td>\n",
       "      <td>0.685427</td>\n",
       "      <td>0.657646</td>\n",
       "      <td>0.660365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.042900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.589953</td>\n",
       "      <td>0.717286</td>\n",
       "      <td>0.743157</td>\n",
       "      <td>0.768552</td>\n",
       "      <td>0.589953</td>\n",
       "      <td>0.239095</td>\n",
       "      <td>0.148631</td>\n",
       "      <td>0.076855</td>\n",
       "      <td>0.589953</td>\n",
       "      <td>0.717286</td>\n",
       "      <td>0.743157</td>\n",
       "      <td>0.768552</td>\n",
       "      <td>0.685031</td>\n",
       "      <td>0.657651</td>\n",
       "      <td>0.660485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.008800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.587777</td>\n",
       "      <td>0.718315</td>\n",
       "      <td>0.744541</td>\n",
       "      <td>0.769541</td>\n",
       "      <td>0.587777</td>\n",
       "      <td>0.239438</td>\n",
       "      <td>0.148908</td>\n",
       "      <td>0.076954</td>\n",
       "      <td>0.587777</td>\n",
       "      <td>0.718315</td>\n",
       "      <td>0.744541</td>\n",
       "      <td>0.769541</td>\n",
       "      <td>0.684857</td>\n",
       "      <td>0.657055</td>\n",
       "      <td>0.659855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.012900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.594778</td>\n",
       "      <td>0.718434</td>\n",
       "      <td>0.743315</td>\n",
       "      <td>0.768473</td>\n",
       "      <td>0.594778</td>\n",
       "      <td>0.239478</td>\n",
       "      <td>0.148663</td>\n",
       "      <td>0.076847</td>\n",
       "      <td>0.594778</td>\n",
       "      <td>0.718434</td>\n",
       "      <td>0.743315</td>\n",
       "      <td>0.768473</td>\n",
       "      <td>0.687287</td>\n",
       "      <td>0.660670</td>\n",
       "      <td>0.663517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.988000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.592366</td>\n",
       "      <td>0.720174</td>\n",
       "      <td>0.744976</td>\n",
       "      <td>0.770214</td>\n",
       "      <td>0.592366</td>\n",
       "      <td>0.240058</td>\n",
       "      <td>0.148995</td>\n",
       "      <td>0.077021</td>\n",
       "      <td>0.592366</td>\n",
       "      <td>0.720174</td>\n",
       "      <td>0.744976</td>\n",
       "      <td>0.770214</td>\n",
       "      <td>0.687390</td>\n",
       "      <td>0.660211</td>\n",
       "      <td>0.663038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.041300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593513</td>\n",
       "      <td>0.721361</td>\n",
       "      <td>0.746400</td>\n",
       "      <td>0.771440</td>\n",
       "      <td>0.593513</td>\n",
       "      <td>0.240454</td>\n",
       "      <td>0.149280</td>\n",
       "      <td>0.077144</td>\n",
       "      <td>0.593513</td>\n",
       "      <td>0.721361</td>\n",
       "      <td>0.746400</td>\n",
       "      <td>0.771440</td>\n",
       "      <td>0.688237</td>\n",
       "      <td>0.660951</td>\n",
       "      <td>0.663748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593157</td>\n",
       "      <td>0.722508</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.771479</td>\n",
       "      <td>0.593157</td>\n",
       "      <td>0.240836</td>\n",
       "      <td>0.149343</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.593157</td>\n",
       "      <td>0.722508</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.771479</td>\n",
       "      <td>0.688519</td>\n",
       "      <td>0.661268</td>\n",
       "      <td>0.664065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.992900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593592</td>\n",
       "      <td>0.721479</td>\n",
       "      <td>0.747033</td>\n",
       "      <td>0.771559</td>\n",
       "      <td>0.593592</td>\n",
       "      <td>0.240493</td>\n",
       "      <td>0.149407</td>\n",
       "      <td>0.077156</td>\n",
       "      <td>0.593592</td>\n",
       "      <td>0.721479</td>\n",
       "      <td>0.747033</td>\n",
       "      <td>0.771559</td>\n",
       "      <td>0.688641</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.664189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.940300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.720886</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.771677</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.240295</td>\n",
       "      <td>0.149343</td>\n",
       "      <td>0.077168</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.720886</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.771677</td>\n",
       "      <td>0.689853</td>\n",
       "      <td>0.663022</td>\n",
       "      <td>0.665813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.978900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596282</td>\n",
       "      <td>0.723062</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.773101</td>\n",
       "      <td>0.596282</td>\n",
       "      <td>0.241021</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>0.077310</td>\n",
       "      <td>0.596282</td>\n",
       "      <td>0.723062</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.773101</td>\n",
       "      <td>0.690744</td>\n",
       "      <td>0.663716</td>\n",
       "      <td>0.666452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.959500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.597547</td>\n",
       "      <td>0.723022</td>\n",
       "      <td>0.747943</td>\n",
       "      <td>0.772983</td>\n",
       "      <td>0.597547</td>\n",
       "      <td>0.241007</td>\n",
       "      <td>0.149589</td>\n",
       "      <td>0.077298</td>\n",
       "      <td>0.597547</td>\n",
       "      <td>0.723022</td>\n",
       "      <td>0.747943</td>\n",
       "      <td>0.772983</td>\n",
       "      <td>0.691222</td>\n",
       "      <td>0.664399</td>\n",
       "      <td>0.667179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.978600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.723813</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.241271</td>\n",
       "      <td>0.149707</td>\n",
       "      <td>0.077354</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.723813</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.691413</td>\n",
       "      <td>0.664456</td>\n",
       "      <td>0.667252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.723497</td>\n",
       "      <td>0.748655</td>\n",
       "      <td>0.773457</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.241166</td>\n",
       "      <td>0.149731</td>\n",
       "      <td>0.077346</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.723497</td>\n",
       "      <td>0.748655</td>\n",
       "      <td>0.773457</td>\n",
       "      <td>0.691061</td>\n",
       "      <td>0.664013</td>\n",
       "      <td>0.666813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.924500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593869</td>\n",
       "      <td>0.723378</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.772587</td>\n",
       "      <td>0.593869</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>0.077259</td>\n",
       "      <td>0.593869</td>\n",
       "      <td>0.723378</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.772587</td>\n",
       "      <td>0.689694</td>\n",
       "      <td>0.662437</td>\n",
       "      <td>0.665314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.968500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.723932</td>\n",
       "      <td>0.748734</td>\n",
       "      <td>0.773774</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.241311</td>\n",
       "      <td>0.149747</td>\n",
       "      <td>0.077377</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.723932</td>\n",
       "      <td>0.748734</td>\n",
       "      <td>0.773774</td>\n",
       "      <td>0.690560</td>\n",
       "      <td>0.663225</td>\n",
       "      <td>0.666030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.592801</td>\n",
       "      <td>0.723695</td>\n",
       "      <td>0.748259</td>\n",
       "      <td>0.773616</td>\n",
       "      <td>0.592801</td>\n",
       "      <td>0.241232</td>\n",
       "      <td>0.149652</td>\n",
       "      <td>0.077362</td>\n",
       "      <td>0.592801</td>\n",
       "      <td>0.723695</td>\n",
       "      <td>0.748259</td>\n",
       "      <td>0.773616</td>\n",
       "      <td>0.689553</td>\n",
       "      <td>0.661936</td>\n",
       "      <td>0.664752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.939000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.595293</td>\n",
       "      <td>0.723892</td>\n",
       "      <td>0.748576</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.595293</td>\n",
       "      <td>0.241297</td>\n",
       "      <td>0.149715</td>\n",
       "      <td>0.077354</td>\n",
       "      <td>0.595293</td>\n",
       "      <td>0.723892</td>\n",
       "      <td>0.748576</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.690647</td>\n",
       "      <td>0.663421</td>\n",
       "      <td>0.666274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.982200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.594264</td>\n",
       "      <td>0.723932</td>\n",
       "      <td>0.749525</td>\n",
       "      <td>0.773932</td>\n",
       "      <td>0.594264</td>\n",
       "      <td>0.241311</td>\n",
       "      <td>0.149905</td>\n",
       "      <td>0.077393</td>\n",
       "      <td>0.594264</td>\n",
       "      <td>0.723932</td>\n",
       "      <td>0.749525</td>\n",
       "      <td>0.773932</td>\n",
       "      <td>0.690446</td>\n",
       "      <td>0.663011</td>\n",
       "      <td>0.665855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.003800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.595886</td>\n",
       "      <td>0.724644</td>\n",
       "      <td>0.749328</td>\n",
       "      <td>0.774090</td>\n",
       "      <td>0.595886</td>\n",
       "      <td>0.241548</td>\n",
       "      <td>0.149866</td>\n",
       "      <td>0.077409</td>\n",
       "      <td>0.595886</td>\n",
       "      <td>0.724644</td>\n",
       "      <td>0.749328</td>\n",
       "      <td>0.774090</td>\n",
       "      <td>0.691257</td>\n",
       "      <td>0.664049</td>\n",
       "      <td>0.666895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.595332</td>\n",
       "      <td>0.724723</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.773932</td>\n",
       "      <td>0.595332</td>\n",
       "      <td>0.241574</td>\n",
       "      <td>0.149913</td>\n",
       "      <td>0.077393</td>\n",
       "      <td>0.595332</td>\n",
       "      <td>0.724723</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.773932</td>\n",
       "      <td>0.690979</td>\n",
       "      <td>0.663714</td>\n",
       "      <td>0.666593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596203</td>\n",
       "      <td>0.724960</td>\n",
       "      <td>0.749407</td>\n",
       "      <td>0.774209</td>\n",
       "      <td>0.596203</td>\n",
       "      <td>0.241653</td>\n",
       "      <td>0.149881</td>\n",
       "      <td>0.077421</td>\n",
       "      <td>0.596203</td>\n",
       "      <td>0.724960</td>\n",
       "      <td>0.749407</td>\n",
       "      <td>0.774209</td>\n",
       "      <td>0.691451</td>\n",
       "      <td>0.664269</td>\n",
       "      <td>0.667130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.725277</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.774328</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.241759</td>\n",
       "      <td>0.149913</td>\n",
       "      <td>0.077433</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.725277</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.774328</td>\n",
       "      <td>0.691938</td>\n",
       "      <td>0.664875</td>\n",
       "      <td>0.667724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3555, training_loss=1.0422707043954926, metrics={'train_runtime': 4517.6472, 'train_samples_per_second': 50.362, 'train_steps_per_second': 0.787, 'total_flos': 0.0, 'train_loss': 1.0422707043954926, 'epoch': 1.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0aae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODEL_REPO=\"\"\n",
    "model.save_pretrained(FINAL_MODEL_REPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f34f6",
   "metadata": {},
   "source": [
    "## HuggingFace Hub Push\n",
    "\n",
    "The trained model is pushed to the HuggingFace Hub for sharing and deployment. This allows the model to be easily loaded and used by others using the SentenceTransformer library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf614a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec784f7fb59e451885a51e5997f138a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "login()\n",
    "REPO_ID = \"\"\n",
    "model.push_to_hub(repo_id=REPO_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrabble-embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
