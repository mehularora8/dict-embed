{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e5b96c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b03db48b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (2.3.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "917aa221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data/csw24.txt and convert line by line to csv\n",
    "# each line is word   definition.\n",
    "# convert to csv with two columns: word and definition\n",
    "import pandas as pd\n",
    "\n",
    "with open('data/csw24.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# each line is word<tab>definition.\n",
    "# convert to csv with two columns: word and definition\n",
    "data = []\n",
    "for line in lines:\n",
    "    word, definition = line.strip().split('\\t', 1)\n",
    "    data.append({'word': word, 'definition': definition})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "assert len(df) == len(lines)\n",
    "df.to_csv('data/csw24.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68854f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses, SentenceTransformerTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0294f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "# MODEL_NAME = 'Qwen/Qwen3-Embedding-0.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "108c890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1896433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.max_seq_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d274a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_loss = losses.MultipleNegativesRankingLoss(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e264abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Target dims should contain the model's dimensions.\n",
    "target_dims = [384, 256]\n",
    "mrl_loss = losses.MatryoshkaLoss(model, base_loss, target_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4786c305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227518 25280 28089\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"data/csw24.csv\")\n",
    "\n",
    "splits = dataset['train'].train_test_split(test_size=0.1)\n",
    "test_dataset = splits['test']\n",
    "train_val_dataset = splits['train']\n",
    "train_val_splits = train_val_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_val_splits['train']\n",
    "val_dataset = train_val_splits['test']\n",
    "\n",
    "\n",
    "print(len(train_dataset), len(val_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ad3bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "evaluator = InformationRetrievalEvaluator(\n",
    "    queries={i: example['word'] for i, example in enumerate(val_dataset)},\n",
    "    corpus={i: example['definition'] for i, example in enumerate(val_dataset)},\n",
    "    relevant_docs={i: [i] for i in range(len(val_dataset))}, # Word i's def is always doc i\n",
    "    name='dictionary-test'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d92175f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "\n",
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir='./output',\n",
    "    per_device_train_batch_size=64,  # batch size\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-5,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=mrl_loss,\n",
    "    args=training_args,\n",
    "    evaluator=evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8e8059a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arora\\OneDrive\\Desktop\\Dev\\scrabble-embed\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3555' max='3555' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3555/3555 1:15:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Dictionary-test Cosine Accuracy@1</th>\n",
       "      <th>Dictionary-test Cosine Accuracy@3</th>\n",
       "      <th>Dictionary-test Cosine Accuracy@5</th>\n",
       "      <th>Dictionary-test Cosine Accuracy@10</th>\n",
       "      <th>Dictionary-test Cosine Precision@1</th>\n",
       "      <th>Dictionary-test Cosine Precision@3</th>\n",
       "      <th>Dictionary-test Cosine Precision@5</th>\n",
       "      <th>Dictionary-test Cosine Precision@10</th>\n",
       "      <th>Dictionary-test Cosine Recall@1</th>\n",
       "      <th>Dictionary-test Cosine Recall@3</th>\n",
       "      <th>Dictionary-test Cosine Recall@5</th>\n",
       "      <th>Dictionary-test Cosine Recall@10</th>\n",
       "      <th>Dictionary-test Cosine Ndcg@10</th>\n",
       "      <th>Dictionary-test Cosine Mrr@10</th>\n",
       "      <th>Dictionary-test Cosine Map@100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.535300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>0.670570</td>\n",
       "      <td>0.709771</td>\n",
       "      <td>0.743513</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>0.223523</td>\n",
       "      <td>0.141954</td>\n",
       "      <td>0.074351</td>\n",
       "      <td>0.504826</td>\n",
       "      <td>0.670570</td>\n",
       "      <td>0.709771</td>\n",
       "      <td>0.743513</td>\n",
       "      <td>0.630627</td>\n",
       "      <td>0.593713</td>\n",
       "      <td>0.596473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.283600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.691377</td>\n",
       "      <td>0.720807</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.230459</td>\n",
       "      <td>0.144161</td>\n",
       "      <td>0.074854</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.691377</td>\n",
       "      <td>0.720807</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.654278</td>\n",
       "      <td>0.623322</td>\n",
       "      <td>0.626132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.230500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.559217</td>\n",
       "      <td>0.698972</td>\n",
       "      <td>0.727532</td>\n",
       "      <td>0.755617</td>\n",
       "      <td>0.559217</td>\n",
       "      <td>0.232991</td>\n",
       "      <td>0.145506</td>\n",
       "      <td>0.075562</td>\n",
       "      <td>0.559217</td>\n",
       "      <td>0.698972</td>\n",
       "      <td>0.727532</td>\n",
       "      <td>0.755617</td>\n",
       "      <td>0.663688</td>\n",
       "      <td>0.633532</td>\n",
       "      <td>0.636208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.166900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.560680</td>\n",
       "      <td>0.700356</td>\n",
       "      <td>0.729945</td>\n",
       "      <td>0.757081</td>\n",
       "      <td>0.560680</td>\n",
       "      <td>0.233452</td>\n",
       "      <td>0.145989</td>\n",
       "      <td>0.075708</td>\n",
       "      <td>0.560680</td>\n",
       "      <td>0.700356</td>\n",
       "      <td>0.729945</td>\n",
       "      <td>0.757081</td>\n",
       "      <td>0.665149</td>\n",
       "      <td>0.634996</td>\n",
       "      <td>0.637667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.190400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.572271</td>\n",
       "      <td>0.704905</td>\n",
       "      <td>0.732239</td>\n",
       "      <td>0.758109</td>\n",
       "      <td>0.572271</td>\n",
       "      <td>0.234968</td>\n",
       "      <td>0.146448</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.572271</td>\n",
       "      <td>0.704905</td>\n",
       "      <td>0.732239</td>\n",
       "      <td>0.758109</td>\n",
       "      <td>0.671440</td>\n",
       "      <td>0.642986</td>\n",
       "      <td>0.645754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.099800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.573774</td>\n",
       "      <td>0.707951</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761709</td>\n",
       "      <td>0.573774</td>\n",
       "      <td>0.235984</td>\n",
       "      <td>0.146875</td>\n",
       "      <td>0.076171</td>\n",
       "      <td>0.573774</td>\n",
       "      <td>0.707951</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.761709</td>\n",
       "      <td>0.673844</td>\n",
       "      <td>0.645033</td>\n",
       "      <td>0.647714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.065500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.575277</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.735285</td>\n",
       "      <td>0.762658</td>\n",
       "      <td>0.575277</td>\n",
       "      <td>0.236287</td>\n",
       "      <td>0.147057</td>\n",
       "      <td>0.076266</td>\n",
       "      <td>0.575277</td>\n",
       "      <td>0.708861</td>\n",
       "      <td>0.735285</td>\n",
       "      <td>0.762658</td>\n",
       "      <td>0.675059</td>\n",
       "      <td>0.646343</td>\n",
       "      <td>0.649096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.095000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.580934</td>\n",
       "      <td>0.711472</td>\n",
       "      <td>0.735997</td>\n",
       "      <td>0.763252</td>\n",
       "      <td>0.580934</td>\n",
       "      <td>0.237157</td>\n",
       "      <td>0.147199</td>\n",
       "      <td>0.076325</td>\n",
       "      <td>0.580934</td>\n",
       "      <td>0.711472</td>\n",
       "      <td>0.735997</td>\n",
       "      <td>0.763252</td>\n",
       "      <td>0.678071</td>\n",
       "      <td>0.650149</td>\n",
       "      <td>0.652928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.153500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.585839</td>\n",
       "      <td>0.712658</td>\n",
       "      <td>0.738924</td>\n",
       "      <td>0.765427</td>\n",
       "      <td>0.585839</td>\n",
       "      <td>0.237553</td>\n",
       "      <td>0.147785</td>\n",
       "      <td>0.076543</td>\n",
       "      <td>0.585839</td>\n",
       "      <td>0.712658</td>\n",
       "      <td>0.738924</td>\n",
       "      <td>0.765427</td>\n",
       "      <td>0.681258</td>\n",
       "      <td>0.653691</td>\n",
       "      <td>0.656405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.004700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.586432</td>\n",
       "      <td>0.713331</td>\n",
       "      <td>0.737540</td>\n",
       "      <td>0.765071</td>\n",
       "      <td>0.586432</td>\n",
       "      <td>0.237777</td>\n",
       "      <td>0.147508</td>\n",
       "      <td>0.076507</td>\n",
       "      <td>0.586432</td>\n",
       "      <td>0.713331</td>\n",
       "      <td>0.737540</td>\n",
       "      <td>0.765071</td>\n",
       "      <td>0.681367</td>\n",
       "      <td>0.653972</td>\n",
       "      <td>0.656740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.074900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.583070</td>\n",
       "      <td>0.714517</td>\n",
       "      <td>0.739992</td>\n",
       "      <td>0.766495</td>\n",
       "      <td>0.583070</td>\n",
       "      <td>0.238172</td>\n",
       "      <td>0.147998</td>\n",
       "      <td>0.076650</td>\n",
       "      <td>0.583070</td>\n",
       "      <td>0.714517</td>\n",
       "      <td>0.739992</td>\n",
       "      <td>0.766495</td>\n",
       "      <td>0.680913</td>\n",
       "      <td>0.652843</td>\n",
       "      <td>0.655654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.064200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.586353</td>\n",
       "      <td>0.712737</td>\n",
       "      <td>0.738687</td>\n",
       "      <td>0.764399</td>\n",
       "      <td>0.586353</td>\n",
       "      <td>0.237579</td>\n",
       "      <td>0.147737</td>\n",
       "      <td>0.076440</td>\n",
       "      <td>0.586353</td>\n",
       "      <td>0.712737</td>\n",
       "      <td>0.738687</td>\n",
       "      <td>0.764399</td>\n",
       "      <td>0.681294</td>\n",
       "      <td>0.654044</td>\n",
       "      <td>0.656927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.071800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.590190</td>\n",
       "      <td>0.717049</td>\n",
       "      <td>0.742880</td>\n",
       "      <td>0.768275</td>\n",
       "      <td>0.590190</td>\n",
       "      <td>0.239016</td>\n",
       "      <td>0.148576</td>\n",
       "      <td>0.076828</td>\n",
       "      <td>0.590190</td>\n",
       "      <td>0.717049</td>\n",
       "      <td>0.742880</td>\n",
       "      <td>0.768275</td>\n",
       "      <td>0.685073</td>\n",
       "      <td>0.657792</td>\n",
       "      <td>0.660538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.023000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.588410</td>\n",
       "      <td>0.719581</td>\n",
       "      <td>0.744343</td>\n",
       "      <td>0.770095</td>\n",
       "      <td>0.588410</td>\n",
       "      <td>0.239860</td>\n",
       "      <td>0.148869</td>\n",
       "      <td>0.077009</td>\n",
       "      <td>0.588410</td>\n",
       "      <td>0.719581</td>\n",
       "      <td>0.744343</td>\n",
       "      <td>0.770095</td>\n",
       "      <td>0.685427</td>\n",
       "      <td>0.657646</td>\n",
       "      <td>0.660365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.042900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.589953</td>\n",
       "      <td>0.717286</td>\n",
       "      <td>0.743157</td>\n",
       "      <td>0.768552</td>\n",
       "      <td>0.589953</td>\n",
       "      <td>0.239095</td>\n",
       "      <td>0.148631</td>\n",
       "      <td>0.076855</td>\n",
       "      <td>0.589953</td>\n",
       "      <td>0.717286</td>\n",
       "      <td>0.743157</td>\n",
       "      <td>0.768552</td>\n",
       "      <td>0.685031</td>\n",
       "      <td>0.657651</td>\n",
       "      <td>0.660485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.008800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.587777</td>\n",
       "      <td>0.718315</td>\n",
       "      <td>0.744541</td>\n",
       "      <td>0.769541</td>\n",
       "      <td>0.587777</td>\n",
       "      <td>0.239438</td>\n",
       "      <td>0.148908</td>\n",
       "      <td>0.076954</td>\n",
       "      <td>0.587777</td>\n",
       "      <td>0.718315</td>\n",
       "      <td>0.744541</td>\n",
       "      <td>0.769541</td>\n",
       "      <td>0.684857</td>\n",
       "      <td>0.657055</td>\n",
       "      <td>0.659855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.012900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.594778</td>\n",
       "      <td>0.718434</td>\n",
       "      <td>0.743315</td>\n",
       "      <td>0.768473</td>\n",
       "      <td>0.594778</td>\n",
       "      <td>0.239478</td>\n",
       "      <td>0.148663</td>\n",
       "      <td>0.076847</td>\n",
       "      <td>0.594778</td>\n",
       "      <td>0.718434</td>\n",
       "      <td>0.743315</td>\n",
       "      <td>0.768473</td>\n",
       "      <td>0.687287</td>\n",
       "      <td>0.660670</td>\n",
       "      <td>0.663517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.988000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.592366</td>\n",
       "      <td>0.720174</td>\n",
       "      <td>0.744976</td>\n",
       "      <td>0.770214</td>\n",
       "      <td>0.592366</td>\n",
       "      <td>0.240058</td>\n",
       "      <td>0.148995</td>\n",
       "      <td>0.077021</td>\n",
       "      <td>0.592366</td>\n",
       "      <td>0.720174</td>\n",
       "      <td>0.744976</td>\n",
       "      <td>0.770214</td>\n",
       "      <td>0.687390</td>\n",
       "      <td>0.660211</td>\n",
       "      <td>0.663038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.041300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593513</td>\n",
       "      <td>0.721361</td>\n",
       "      <td>0.746400</td>\n",
       "      <td>0.771440</td>\n",
       "      <td>0.593513</td>\n",
       "      <td>0.240454</td>\n",
       "      <td>0.149280</td>\n",
       "      <td>0.077144</td>\n",
       "      <td>0.593513</td>\n",
       "      <td>0.721361</td>\n",
       "      <td>0.746400</td>\n",
       "      <td>0.771440</td>\n",
       "      <td>0.688237</td>\n",
       "      <td>0.660951</td>\n",
       "      <td>0.663748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.004300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593157</td>\n",
       "      <td>0.722508</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.771479</td>\n",
       "      <td>0.593157</td>\n",
       "      <td>0.240836</td>\n",
       "      <td>0.149343</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>0.593157</td>\n",
       "      <td>0.722508</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.771479</td>\n",
       "      <td>0.688519</td>\n",
       "      <td>0.661268</td>\n",
       "      <td>0.664065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.992900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593592</td>\n",
       "      <td>0.721479</td>\n",
       "      <td>0.747033</td>\n",
       "      <td>0.771559</td>\n",
       "      <td>0.593592</td>\n",
       "      <td>0.240493</td>\n",
       "      <td>0.149407</td>\n",
       "      <td>0.077156</td>\n",
       "      <td>0.593592</td>\n",
       "      <td>0.721479</td>\n",
       "      <td>0.747033</td>\n",
       "      <td>0.771559</td>\n",
       "      <td>0.688641</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.664189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.940300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.720886</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.771677</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.240295</td>\n",
       "      <td>0.149343</td>\n",
       "      <td>0.077168</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.720886</td>\n",
       "      <td>0.746717</td>\n",
       "      <td>0.771677</td>\n",
       "      <td>0.689853</td>\n",
       "      <td>0.663022</td>\n",
       "      <td>0.665813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.978900</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596282</td>\n",
       "      <td>0.723062</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.773101</td>\n",
       "      <td>0.596282</td>\n",
       "      <td>0.241021</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>0.077310</td>\n",
       "      <td>0.596282</td>\n",
       "      <td>0.723062</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.773101</td>\n",
       "      <td>0.690744</td>\n",
       "      <td>0.663716</td>\n",
       "      <td>0.666452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.959500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.597547</td>\n",
       "      <td>0.723022</td>\n",
       "      <td>0.747943</td>\n",
       "      <td>0.772983</td>\n",
       "      <td>0.597547</td>\n",
       "      <td>0.241007</td>\n",
       "      <td>0.149589</td>\n",
       "      <td>0.077298</td>\n",
       "      <td>0.597547</td>\n",
       "      <td>0.723022</td>\n",
       "      <td>0.747943</td>\n",
       "      <td>0.772983</td>\n",
       "      <td>0.691222</td>\n",
       "      <td>0.664399</td>\n",
       "      <td>0.667179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.978600</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.723813</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.241271</td>\n",
       "      <td>0.149707</td>\n",
       "      <td>0.077354</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.723813</td>\n",
       "      <td>0.748536</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.691413</td>\n",
       "      <td>0.664456</td>\n",
       "      <td>0.667252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.964700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.723497</td>\n",
       "      <td>0.748655</td>\n",
       "      <td>0.773457</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.241166</td>\n",
       "      <td>0.149731</td>\n",
       "      <td>0.077346</td>\n",
       "      <td>0.596400</td>\n",
       "      <td>0.723497</td>\n",
       "      <td>0.748655</td>\n",
       "      <td>0.773457</td>\n",
       "      <td>0.691061</td>\n",
       "      <td>0.664013</td>\n",
       "      <td>0.666813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.924500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.593869</td>\n",
       "      <td>0.723378</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.772587</td>\n",
       "      <td>0.593869</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>0.077259</td>\n",
       "      <td>0.593869</td>\n",
       "      <td>0.723378</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.772587</td>\n",
       "      <td>0.689694</td>\n",
       "      <td>0.662437</td>\n",
       "      <td>0.665314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.968500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.723932</td>\n",
       "      <td>0.748734</td>\n",
       "      <td>0.773774</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.241311</td>\n",
       "      <td>0.149747</td>\n",
       "      <td>0.077377</td>\n",
       "      <td>0.594739</td>\n",
       "      <td>0.723932</td>\n",
       "      <td>0.748734</td>\n",
       "      <td>0.773774</td>\n",
       "      <td>0.690560</td>\n",
       "      <td>0.663225</td>\n",
       "      <td>0.666030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.977800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.592801</td>\n",
       "      <td>0.723695</td>\n",
       "      <td>0.748259</td>\n",
       "      <td>0.773616</td>\n",
       "      <td>0.592801</td>\n",
       "      <td>0.241232</td>\n",
       "      <td>0.149652</td>\n",
       "      <td>0.077362</td>\n",
       "      <td>0.592801</td>\n",
       "      <td>0.723695</td>\n",
       "      <td>0.748259</td>\n",
       "      <td>0.773616</td>\n",
       "      <td>0.689553</td>\n",
       "      <td>0.661936</td>\n",
       "      <td>0.664752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.939000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.595293</td>\n",
       "      <td>0.723892</td>\n",
       "      <td>0.748576</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.595293</td>\n",
       "      <td>0.241297</td>\n",
       "      <td>0.149715</td>\n",
       "      <td>0.077354</td>\n",
       "      <td>0.595293</td>\n",
       "      <td>0.723892</td>\n",
       "      <td>0.748576</td>\n",
       "      <td>0.773536</td>\n",
       "      <td>0.690647</td>\n",
       "      <td>0.663421</td>\n",
       "      <td>0.666274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.982200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.594264</td>\n",
       "      <td>0.723932</td>\n",
       "      <td>0.749525</td>\n",
       "      <td>0.773932</td>\n",
       "      <td>0.594264</td>\n",
       "      <td>0.241311</td>\n",
       "      <td>0.149905</td>\n",
       "      <td>0.077393</td>\n",
       "      <td>0.594264</td>\n",
       "      <td>0.723932</td>\n",
       "      <td>0.749525</td>\n",
       "      <td>0.773932</td>\n",
       "      <td>0.690446</td>\n",
       "      <td>0.663011</td>\n",
       "      <td>0.665855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.003800</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.595886</td>\n",
       "      <td>0.724644</td>\n",
       "      <td>0.749328</td>\n",
       "      <td>0.774090</td>\n",
       "      <td>0.595886</td>\n",
       "      <td>0.241548</td>\n",
       "      <td>0.149866</td>\n",
       "      <td>0.077409</td>\n",
       "      <td>0.595886</td>\n",
       "      <td>0.724644</td>\n",
       "      <td>0.749328</td>\n",
       "      <td>0.774090</td>\n",
       "      <td>0.691257</td>\n",
       "      <td>0.664049</td>\n",
       "      <td>0.666895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.929700</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.595332</td>\n",
       "      <td>0.724723</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.773932</td>\n",
       "      <td>0.595332</td>\n",
       "      <td>0.241574</td>\n",
       "      <td>0.149913</td>\n",
       "      <td>0.077393</td>\n",
       "      <td>0.595332</td>\n",
       "      <td>0.724723</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.773932</td>\n",
       "      <td>0.690979</td>\n",
       "      <td>0.663714</td>\n",
       "      <td>0.666593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.596203</td>\n",
       "      <td>0.724960</td>\n",
       "      <td>0.749407</td>\n",
       "      <td>0.774209</td>\n",
       "      <td>0.596203</td>\n",
       "      <td>0.241653</td>\n",
       "      <td>0.149881</td>\n",
       "      <td>0.077421</td>\n",
       "      <td>0.596203</td>\n",
       "      <td>0.724960</td>\n",
       "      <td>0.749407</td>\n",
       "      <td>0.774209</td>\n",
       "      <td>0.691451</td>\n",
       "      <td>0.664269</td>\n",
       "      <td>0.667130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.948000</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.725277</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.774328</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.241759</td>\n",
       "      <td>0.149913</td>\n",
       "      <td>0.077433</td>\n",
       "      <td>0.597033</td>\n",
       "      <td>0.725277</td>\n",
       "      <td>0.749565</td>\n",
       "      <td>0.774328</td>\n",
       "      <td>0.691938</td>\n",
       "      <td>0.664875</td>\n",
       "      <td>0.667724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3555, training_loss=1.0422707043954926, metrics={'train_runtime': 4517.6472, 'train_samples_per_second': 50.362, 'train_steps_per_second': 0.787, 'total_flos': 0.0, 'train_loss': 1.0422707043954926, 'epoch': 1.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c0aae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/scrabble-embed/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fdaf96cd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from ipywidgets) (9.8.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama>=0.4.4 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator>=4.3.2 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\arora\\onedrive\\desktop\\dev\\scrabble-embed\\.venv\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "   ---------------------------------------- 0.0/914.9 kB ? eta -:--:--\n",
      "   ---------------------- ----------------- 524.3/914.9 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 914.9/914.9 kB 3.8 MB/s  0:00:00\n",
      "Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.5/2.2 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.2 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.3 MB/s  0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   ---------------------------------------- 3/3 [ipywidgets]\n",
      "\n",
      "Successfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "21cf614a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec784f7fb59e451885a51e5997f138a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "73b9c272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|| 90.9M/90.9M [01:17<00:00, 1.17MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/mehularora/scrabble-embed-v1/commit/6928d152581e6be299af0c53e4e0d48668c22622'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(repo_id='Mehularora/scrabble-embed-v1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
